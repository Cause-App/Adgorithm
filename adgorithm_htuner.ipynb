{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "\n",
    "from keras.layers import Input, Dense, Concatenate, GlobalAveragePooling1D, Dropout, Lambda\n",
    "from IPython.display import display, Image, update_display, HTML\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from transformer import TransformerBlock\n",
    "from keras.regularizers import L1L2\n",
    "import user_ad_interaction\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import users\n",
    "import tools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directories = [\"ADS16_Benchmark_part1\", \"ADS16_Benchmark_part2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tuner = tools.HyperbandWithBatchSize(\n",
    "        user_ad_interaction.create_model,\n",
    "        objective=kt.Objective(\"val_rating_mae\", direction=\"min\"),\n",
    "        max_epochs=100,\n",
    "        factor=3,\n",
    "        hyperband_iterations=5,\n",
    "        directory=\"user_ad_interaction_model_logs/hypertraining\",\n",
    "        project_name=\"user_ad_interaction\"\n",
    "    )\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    user_model = user_ad_interaction.create_model(best_hps)\n",
    "    user_model.load_weights(\"user_ad_interaction_model_logs/checkpoints/model-000020-1.202267.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ad_ftrs, num_categories, real_user_ftrs, _ = user_ad_interaction.load_user_and_ad_ftrs(root_directories)\n",
    "    user_pca_normal_params = users.approximate_normal_params(real_user_ftrs.T)\n",
    "    n_ads = len(ad_ftrs[0])\n",
    "\n",
    "    rating_to_exp_ctr = {\n",
    "        1: 0.014925,\n",
    "        2: 0.024786,\n",
    "        3: 0.031071,\n",
    "        4: 0.040562,\n",
    "        5: 0.068341\n",
    "    }\n",
    "\n",
    "    n_users = 5\n",
    "    n_clicks = 10\n",
    "    n_rounds = 1\n",
    "    diminishing_returns_fac = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logs_dir=\"adgorithm_logs\"\n",
    "    log_dir = os.path.join(logs_dir, \"fit\", datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_binary_crossentropy(y_true, y_pred):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    losses = tf.map_fn(\n",
    "        fn=lambda x: bce(x[0], x[1]),\n",
    "        elems=[tf.transpose(y_true), tf.transpose(y_pred)],\n",
    "        fn_output_signature=tf.float32\n",
    "    )\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "def link_up_rl_model(hp, inputs_1, inputs_2, ad_ftrs, idx=None):\n",
    "    idx = \"\" if idx is None else f\"_{idx}\"\n",
    "    input_dropout_rate = 0.5 if hp is None else hp.Choice(\"input_dropout\", values=[0.0, 0.1, 0.3, 0.5])\n",
    "    subnet_1 = inputs_1 if input_dropout_rate == 0 else Dropout(input_dropout_rate, name=f\"flat_input_dropout{idx}\")(inputs_1)\n",
    "    subnet_2 = inputs_2 if input_dropout_rate == 0 else Dropout(input_dropout_rate, name=f\"text_input_dropout{idx}\")(inputs_2)\n",
    "\n",
    "    l1 = 1e-2 if hp is None else hp.Choice(\"l1\", values=[1e-2, 1e-3, 1e-4])\n",
    "    l2 = 1e-2 if hp is None else hp.Choice(\"l2\", values=[1e-2, 1e-3, 1e-4])\n",
    "    reg = L1L2(l1=l1, l2=l2)\n",
    "\n",
    "\n",
    "    num_heads = 2 if hp is None else hp.Int(\"num_attn_heads\", min_value=1, max_value=8, step=1)\n",
    "    ff_dim = 32 if hp is None else hp.Int(\"ff_dim\", min_value=32, max_value=128, step=32)\n",
    "    dropout_rate = 0.5 if hp is None else hp.Choice(\"transformer_dropout_1\", values=[0.0, 0.1, 0.3, 0.5])\n",
    "    subnet_2 = TransformerBlock(embed_dim=ad_ftrs[1].shape[-1], num_heads=num_heads, ff_dim=ff_dim, rate=dropout_rate, name=f\"transformer_block{idx}\")(subnet_2)\n",
    "    subnet_2 = GlobalAveragePooling1D(name=f\"pooling{idx}\")(subnet_2)\n",
    "    dropout_rate = 0.5 if hp is None else hp.Choice(\"transformer_dropout_2\", values=[0.0, 0.1, 0.3, 0.5])\n",
    "    if dropout_rate != 0:\n",
    "        subnet_2 = Dropout(dropout_rate)(subnet_2)\n",
    "\n",
    "    output = Concatenate(name=f\"combined_subnets{idx}\")([subnet_1, subnet_2])\n",
    "\n",
    "    num_hidden_layers = 2 if hp is None else hp.Int(\"num_hidden_layers\", 1, 3)\n",
    "    for i in range(num_hidden_layers):\n",
    "        units = 32 if hp is None else hp.Int(f\"units_{i}\", min_value=32, max_value=128, step=32)\n",
    "        output = Dense(units, activation=\"relu\", kernel_regularizer=reg, name=f\"hidden_{i}{idx}\")(output)\n",
    "        dropout_rate = 0.5 if hp is None else hp.Choice(f\"dropout_{i}\", values=[0.0, 0.1, 0.3, 0.5])\n",
    "        if dropout_rate != 0:\n",
    "            output = Dropout(dropout_rate, name=f\"hidden_dropout_{i}{idx}\")(output)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\", kernel_regularizer=reg, name=f\"output{idx}\")(output)\n",
    "    return output\n",
    "\n",
    "def create_rl_model(hp=None, metrics=[], hypertuning=False, n_users=1):\n",
    "    assert n_users == 1 or hypertuning\n",
    "\n",
    "    ad_ftrs, *_ = user_ad_interaction.load_user_and_ad_ftrs(root_directories)\n",
    "\n",
    "    learning_rate = 1e-2 if hp is None else hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    if hypertuning:\n",
    "        big_inputs_1 = Input((n_users, ad_ftrs[0].shape[1]+1,), name=\"flat_input_stacked\")\n",
    "        big_inputs_2 = Input((n_users, *ad_ftrs[1].shape[1:]), name=\"text_input_stacked\")\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(n_users):\n",
    "            inputs_1 = Lambda(lambda x: x[:, i], name=f\"flat_input_{i}\")(big_inputs_1)\n",
    "            inputs_2 = Lambda(lambda x: x[:, i], name=f\"text_input_{i}\")(big_inputs_2)\n",
    "\n",
    "            output = link_up_rl_model(hp, inputs_1, inputs_2, ad_ftrsidx=i)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output = Concatenate(name=\"output\")(outputs)\n",
    "\n",
    "        model = Model(inputs=[big_inputs_1, big_inputs_2], outputs=[output, output])\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=[mean_binary_crossentropy, \"mse\"],\n",
    "            loss_weights=[1,0],\n",
    "            metrics=metrics\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "    else:\n",
    "        inputs_1 = Input((ad_ftrs[0].shape[1]+1,), name=\"flat_input\")\n",
    "        inputs_2 = Input(ad_ftrs[1].shape[1:], name=\"text_input\")\n",
    "\n",
    "        output = link_up_rl_model(hp, inputs_1, inputs_2, ad_ftrs)\n",
    "\n",
    "        model = Model(inputs=[inputs_1, inputs_2], outputs=[output])\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=metrics\n",
    "        )\n",
    "        return model\n",
    "\n",
    "def create_rl_model_for_htuning(hp=None):\n",
    "    return create_rl_model(hp=hp, metrics={\"output\": \"accuracy\"}, hypertuning=True, n_users=n_users)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tuner = tools.HyperbandWithBatchSize(\n",
    "        create_rl_model_for_htuning,\n",
    "        objective=kt.Objective(\"val_output_1_loss\", direction=\"min\"),\n",
    "        max_epochs=100,\n",
    "        factor=3,\n",
    "        hyperband_iterations=5,\n",
    "        directory=os.path.join(logs_dir, \"hypertraining\"),\n",
    "        project_name=\"adgorithm_htuning\"\n",
    "    )\n",
    "\n",
    "    # print(\"EXAMPLE MODEL --- HYPERPARAMS NOT TUNED\")\n",
    "    # model = create_rl_model_for_htuning()\n",
    "    # model.summary()\n",
    "    # c = tools.ModelDisplayer()\n",
    "    # c.model = model\n",
    "    # c.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    synthetic_user_ftrs = users.generate_synthetic_pca_ftrs(n_users, user_pca_normal_params)\n",
    "    prod = tools.get_user_ad_ftr_product(synthetic_user_ftrs, ad_ftrs)\n",
    "    ctr = user_model.predict(prod)\n",
    "    ctr = ctr.reshape(n_users, n_ads, ctr.shape[-1])\n",
    "    ctr = (np.argmax(ctr, axis=-1) + 1).astype(float)\n",
    "\n",
    "    for k, v in rating_to_exp_ctr.items():\n",
    "        ctr[ctr == k] = v\n",
    "\n",
    "    ctr = np.broadcast_to(ctr[:,:,np.newaxis,np.newaxis], (*ctr.shape, n_clicks, n_rounds))\n",
    "    num_times_clicked = np.indices(ctr.shape)[-2]\n",
    "    diminishing_returns = np.power(diminishing_returns_fac, num_times_clicked)\n",
    "    ctr = ctr * diminishing_returns\n",
    "\n",
    "    expanded_ad_ftrs = (\n",
    "        np.broadcast_to(ad_ftrs[0][np.newaxis, :, np.newaxis, np.newaxis, :], (n_users, ad_ftrs[0].shape[0], n_clicks, n_rounds, ad_ftrs[0].shape[1])),\n",
    "        np.broadcast_to(ad_ftrs[1][np.newaxis, :, np.newaxis, np.newaxis, :, :], (n_users, ad_ftrs[1].shape[0], n_clicks, n_rounds, *ad_ftrs[1].shape[1:])),\n",
    "    )\n",
    "\n",
    "    did_click = np.random.uniform(size=ctr.shape) <= ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_prop = 0.8\n",
    "    total_num = n_users*n_ads*n_rounds\n",
    "    train_num = int(train_prop * total_num)\n",
    "\n",
    "    X = [np.concatenate([expanded_ad_ftrs[0], num_times_clicked[:,:,:,:,np.newaxis]], axis=-1), expanded_ad_ftrs[1]]\n",
    "    Y = did_click\n",
    "    Y_ideal = ctr\n",
    "    # (n_users, n_ads, n_clicks, n_rounds, *FTR_SHAPES)\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    \n",
    "    # (n_users, n_ads, n_clicks, n_rounds)\n",
    "    print(Y.shape)\n",
    "\n",
    "    # (n_users, n_ads, n_clicks, n_rounds)\n",
    "    print(Y_ideal.shape, end=\"\\n\\n\")\n",
    "\n",
    "    X[0] = np.swapaxes(X[0].reshape((n_users, n_ads*n_clicks*n_rounds, *X[0].shape[4:])), 0, 1)\n",
    "    X[1] = np.swapaxes(X[1].reshape((n_users, n_ads*n_clicks*n_rounds, *X[1].shape[4:])), 0, 1)\n",
    "\n",
    "    Y = np.swapaxes(Y.reshape((n_users, n_ads*n_clicks*n_rounds, *Y.shape[4:])), 0, 1)\n",
    "    Y_ideal = np.swapaxes(Y_ideal.reshape((n_users, n_ads*n_clicks*n_rounds, *Y_ideal.shape[4:])), 0, 1)\n",
    "\n",
    "    # (n_ads*n_clicks*n_rounds, n_users, *FTR_SHAPES)\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    \n",
    "    # (n_ads*n_clicks*n_rounds, n_users)\n",
    "    print(Y.shape)\n",
    "\n",
    "    # (n_ads*n_clicks*n_rounds, n_users)\n",
    "    print(Y_ideal.shape)\n",
    "\n",
    "    X_train, X_test = tools.split_all(X, train_num)\n",
    "    Y_train, Y_test = Y[:train_num], Y[train_num:]\n",
    "    Y_ideal_train, Y_ideal_test = Y_ideal[:train_num], Y_ideal[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 100\n",
    "    \n",
    "    try:\n",
    "        tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "        params = {\n",
    "            \"verbose\": 0,\n",
    "            \"shuffle\": True,\n",
    "            \"epochs\": epochs\n",
    "        }\n",
    "\n",
    "\n",
    "        display(HTML(\"<h2>HyperTuning</h2>\"))\n",
    "        tuner.search(\n",
    "            X_train,\n",
    "            (Y_train, Y_ideal_train),\n",
    "            **params,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[tensorboard_callback, tools.Printer(), tools.ModelDisplayer(), EarlyStopping(monitor='val_loss', mode=\"min\", patience=5)]\n",
    "        )\n",
    "\n",
    "\n",
    "        display(HTML(\"<h2>Found Optimal Model!</h2>\"))\n",
    "        num_trials = 1\n",
    "\n",
    "        tuner.results_summary(num_trials=num_trials)\n",
    "        print(\"\\n\")\n",
    "        best_hps=tuner.get_best_hyperparameters(num_trials=num_trials)[0]\n",
    "\n",
    "        model = tuner.hypermodel.build(best_hps)\n",
    "        batch_size = best_hps.Int('batch_size', 32, 256, step=32)\n",
    "        \n",
    "        # model = create_model()\n",
    "        # batch_size = 32\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            **params,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test,Y_test),\n",
    "            callbacks=[tensorboard_callback, tools.ModelDisplayer(), tools.Printer(), EarlyStopping(monitor='val_loss', mode=\"min\", patience=10)]\n",
    "        )\n",
    "\n",
    "        d.plot_now()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ae33797dc82a4e2187cbc0422814e57bd9e53f4d6e12db9194bc01391045ea6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
